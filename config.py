batch_size = 16
d_model = 768
num_layers = 5
num_heads = 5
class_num = 5
d_inner = 32
dropout = 0.3
warm_steps = 2000
fea_num = 7
epochs = 200
PAD = 0
KS = 3

Fea_PLUS = 2

# SIG_LEN = 864
# SIG_LEN = 600

# PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'
# PRE_TRAINED_MODEL_NAME = 'facebook/bart-base'
# PRE_TRAINED_MODEL_NAME = 'roberta-base'
# PRE_TRAINED_MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'
# PRE_TRAINED_MODEL_NAME = 'emilyalsentzer/Bio_Discharge_Summary_BERT'
# PRE_TRAINED_MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'

# FINE_TUNED_LLM_MODEL = 'finetune_llm_model_864'
# FINE_TUNED_LLM_MODEL = 'finetune_llm_model'

outdim_size = class_num

# csv = 'data_864.csv'
# csv = 'data_600.csv'

# model_name = 'trans'
# model_name = 'resnet'
# model_name = 'MLP'
# model_name = 'LSTM'

layers = '5'

